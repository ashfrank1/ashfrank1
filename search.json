[
  {
    "objectID": "Tufte_lecture.html",
    "href": "Tufte_lecture.html",
    "title": "Review of Edward Tufte’s Lecture on Data Visualization (2016)",
    "section": "",
    "text": "Edward Tufte’s lecture underscores the intellectual and practical foundations of data visualization, combining historical perspective, analytical, and a vision for the future of data analysis. Tufte begins with a reminder that data is not solely numbers but poses as a medium for storytelling. His guiding principle is that the purpose of visualization is to support reasoning. Effective displays clarify evidence, reveal causality, and aid in drawing accurate conclusions. Tufte draws on Galileo as an exemplar of what he calls the “thinking eye.” Galileo’s Starry Messenger transformed speculation into “visible certainty,” providing empirical evidence through careful observation. This historical example highlights Tufte’s broader argument: that visualization is style and intellectual honesty. He contrasts this with the dangers of confirmation bias and poor research practices, reminding his audience that the ultimate goal of analysis is “getting it right,” and not producing publishable results. He emphasizes two design principles: showing causality and making smart comparisons. Asking “compared with what?” is central to analysis, and visualization should facilitate that comparison. For example, public health visualizations on vaccination success, which demonstrated over 100 million cases prevented, exemplify how large-scale data, presented well, can counter misinformation and guide public understanding. Tufte also discusses technological advancements that allow analysts to present vast datasets without oversimplification. He critiques “chart junk” and insists that every element of a visualization should carry content, much like the precision and elegance seen in Swiss cartography or Google Maps. For Tufte, excellence in design is not about novelty but about serving cognitive tasks and helping viewers think. Finally, he warns of crises in data analysis, citing high false-positive rates in fields like medical research and economics. His critique of flawed practices, such as the overreliance on fMRI software or the collapse of the Phillips Curve in economics, illustrates the dangers of prioritizing models over evidence. His call is for analysts to maintain an “open mind, but not an empty head” and to entertain new ideas while holding firm to standards of evidence and inference. In sum, Tufte’s lecture weaves together philosophy, design, and ethics. It reminds us that data visualization is about clarity, honesty, and intellectual responsibility. His message remains highly relevant in today’s era of big data and rapid information sharing. Visualization bridges evidence, understanding, and clarifying truth through visual storytelling."
  },
  {
    "objectID": "Projects.html",
    "href": "Projects.html",
    "title": "Projects",
    "section": "",
    "text": "Information Management Database Project\nDatabase Final Project Paper\nDatabase Project Slides\n\n\nMethods of Data Collection and Production Final Project Paper\nMethods of Data Collection and Production Final Presentation\n\n\nPositive Emotions Across Races\nAshleigh’s Data Guide A personalized checklist for handling messy data."
  },
  {
    "objectID": "pitfalls.html",
    "href": "pitfalls.html",
    "title": "Notes on Big Data Analytics: Pitfalls and Overfitting",
    "section": "",
    "text": "i. Big Data Analytics Pitfall\nThe power of uncovering patterns, modelling and serving insights is shown throughout big data analytics. However, size, complexity, and multidimensionality of data sets can lead to problems. A key issue is spurious correlations. With millions of variables and billions of records, purely coincidental patterns may appear statistically significant. Considering these correlations as cause-and-effect relationships can cause predictive models to be wrong and misleading. Another concern is data quality. Large data sets often contain inconsistencies, noise, or biases that can skew analyses. Interpretability can be lost when complex models are built without a clear understanding of why the predictions work, making it harder for decision-makers to trust and act on the results. Without careful validation of data, it can lead to false confidence and flawed policy or business decisions.\nii. Overfitting and Overparameterization\nOverfitting occurs when a model learns the underlying trend in the data and the random noise. This typically happens when a model is too complex, such as having too many parameters relative to the amount of data. An overfit model may perform extremely well on historical (training) data but fail to generalize to new or unseen cases. In big data, overparameterization increases this risk. Overparameterization means using too many features or models that are overly flexible. For example, if a surveillance model includes irrelevant search queries that just happen to align with flu season, like “Oscar nominations”, it may identify seasonal coincidences instead of actual influenza activity. This leads to incorrect predictions when conditions change. To avoid overfitting, analysts rely on methods such as cross-validation, regularization, and out-of-sample testing. The goal is to build models that balance complexity, effectiveness, and generalizability, ensuring they remain useful when applied beyond the original dataset.\nExample from Google Flu Trends: Google Flu Trends was a project that tried to track influenza outbreaks using web search data. While it initially performed well, the model later produced large errors. One reason was that search behavior was influenced by media attention and public concern and not only actual flu cases. This illustrates how big data can reflect social noise, not just health outcomes."
  },
  {
    "objectID": "McKinseys_Charts.html",
    "href": "McKinseys_Charts.html",
    "title": "McKinseys_Charts",
    "section": "",
    "text": "McKinsey provides clean, professional design with consistent typography, color schemes and layouts. The visuals are appealing and well structured. Across both, there are a wide variety of charts and it allows the data type to match the chart type. Every chart is accompanied by text explaining what it shows, why it matters, what the trend or insight is. This combination of visual and narrative elements allows non-specialist audiences to understand the core insights without extensive technical interpretation.\nWeek in Charts provides a more immediate, up-to-date representation of current events or evolving issues. The visuals tend to be straightforward and narrowly focused, prioritizing concise, digestible insights. Year in Charts adopts a more comprehensive and reflective approach, synthesizing information across multiple domains to illustrate broader, long-term developments. The charts in this collection often integrate multiple metrics or layers of data within a single visual, emphasizing cumulative patterns and systemic trends over time.\nTo further innovate, McKinsey could incorporate interactive elements that allow users to toggle between scenarios, time periods, or filters, helping personalize insights. Animated transitions between historical and projected data could enhance storytelling and engagement. Additionally, ensuring color palettes are fully accessible, including colorblind-friendly contrasts and shading, would improve inclusivity and ensure interpretive accuracy across diverse audiences."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ashfrank1.github.io",
    "section": "",
    "text": "Ashleigh Frank\n\nI am currently studying for my MS in Social Data Analytics and Research at UTD. I have a BS in Sociology with a minor in Psychology from UH.\n\nMore Information\nInterests:\n\nData Analysis\nData Science\nStatistics\n\n\n\nContact Me\namf230004@utdallas.edu\nLinkedIn"
  },
  {
    "objectID": "EPPS6354.html",
    "href": "EPPS6354.html",
    "title": "EPPS6354",
    "section": "",
    "text": "This site is used for my Information Management class (EPPS6354).\nAssignment 1\nAssignment 2\nAssignment 3\nAssignment 4\nAssignment 5\nAssignment 6\nDatabase Project Slides\nDatabase Final Project Paper\nhttps://ashfrank1.shinyapps.io/ProjectApplication"
  },
  {
    "objectID": "Data Visualization Resources.html",
    "href": "Data Visualization Resources.html",
    "title": "Data Visualization Resources",
    "section": "",
    "text": "ggplot2: elegant graphics for data analysis\nhttps://guides.library.duke.edu/datavis\nhttps://guides.lib.berkeley.edu/data-visualization\nShiny:\nhttps://shiny.posit.co/r/getstarted/shiny-basics/lesson1/\nShiny::Cheatsheet\nhttps://shiny.posit.co/r/articles/\nShiny Live examples\nR Studio\nhttps://rstats.wtf/index.html\nhttps://p3m.dev/client/#/\nD3 github\nD3 gallery / D3 | Observable\nOther:\nNotion Notes"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\nThis site is used for my accomplishments and projects throughout my masters degree.\n\n\nM.S.\nMy research focuses on various aspects of data science, including the creation of databases and applications, as well as methods for analyzing and visualizing data to extract meaningful insights.\nFall 2025: I am learning how to apply programming to social science automation, including GIS object-based model design and the use of programming languages such as Python for project development. I work with data structures, algorithms, user-interface design, and database management, with an emphasis on rapid application development through hands-on projects. I will design and implement independent projects that allow me to synthesize programming concepts, customize applications, and evaluate methods for processing geospatial data at the object level while aligning programming with my research agenda.\nI am also learning how to design and critique data visualizations, from theoretical foundations such as the grammar of graphics, perception, and cognition to AI-assisted analysis and communication. I program visualizations for both exploration and storytelling, and I will prototype, evaluate, and iterate them with attention to effectiveness, accuracy, ethics, and reproducibility. I build interactive graphics, dashboards, and explorable explanations that communicate insights with ethical guardrails.\n\nPast Semesters\nSpring 2025: I worked on research analysis of Positive Emotions Across Races in my independent study, exploring past literature and data sets. I am using North Texas Quality of Life data to use descriptive statistics and correlational analysis to understand patterns and relationships related to race and positive emotions. The hopes is to create a research paper incorporating regression analysis. In econometrics, I studied regression analysis on the interpretation of Ordinary Least Squares, times series and instrument variables with using STATA. I was nominated for Outstanding Graduate Student.\nFall 2024: My research consisted of designing research, using different methods to obtain experimental or observational data and understanding regression models and how to visual the data through charts or spatially. In Methods of Data Collection and Production, I am worked with 3 people on a mixed method research on fast fast and the impact of consumers through social media influencers. We are used web scarping and survey to do this research. In Research Design, I worked with 2 people on understanding how to design an experiments. We researched pro-climate change information and shifting public consensus. This project consisted of creating a literature review, creating a methodology with directional hypotheses and upholding experimental design. Additionally, we designed a shorter project where we interviewed people in person to understand UTD student’s course load and stress levels. We used some questions from the Perceived Stress Scale and created additional question to know their course load. In Research Design, I gained experience in hypothesis testing, survey design and research methodology.\nSpring 2024: In Information Management, I learned how to design a database using SQL and NoSQL and create an application using RStudio. In methods of qualitative research, I designed research project understanding social benefits of immigrants in host country assimilation in the corporate world. This project consisted of taking field notes, interviewing participants and understanding the IRB process. We hoped this project would receive enough participants to publish research but unfortunately, we did not.\n\n\n\nUndergraduate Studies\nWith BS in Sociology, I obtained a concentration in health and medicine. I looked at disparities among healthcare access through race, gender and income level. I focused a lot of studies towards drug abuse and drug addiction and looked at policies analyses of recidivism for drug incarcerations.\nI conducted research studies at the Social Processes Lab at the University of Houston. The lab focused their studies on how individuals are impacted by social relationships specifically with close relationships.\nSocial Processes Lab"
  },
  {
    "objectID": "Data visualization and data science, Hadley Wickham.html",
    "href": "Data visualization and data science, Hadley Wickham.html",
    "title": "Data visualization and data science, Hadley Wickham",
    "section": "",
    "text": "https://www.youtube.com/watch?v=9YTNYT1maa4\n\nTechnologies / techniques / methods that Wickham introduces or discusses\n\nGrammar of Graphics / ggplot2 / layered plotting\n\nWickham is the author of ggplot2, which implements a grammar of graphics approach to building visualizations (i.e. mapping data → aesthetics → geometric objects → faceting / layers). He emphasizes that using consistent grammar lets one build complex plots via composition of simple building blocks. This “layered / modular” approach is a central piece of his visualization philosophy.\n\nTidy data / data cleaning / structuring.\n\nBefore visualizing, Wickham insists on cleaning, structuring, and “tidying” data (so that each variable is in its own column, each observation in its own row, no ambiguity). This is part of the “data science pipeline” that he often describes (import → tidy → transform → visualize → model / communicate).\n\nStory / message first, then design\n\nDecide the message or story you want to communicate before designing the visual representation. The visual form should serve the message, not the other way around.\n\nDesign principles / visual perception\n\nHe discusses limitations of human visual perception and the need to consider what designs are perceptually effective (e.g. choosing color scales well, avoiding misleading mappings). He warns against blindly trusting default color ramps (e.g. rainbow) and urges more thoughtful design.\nWickham’s talk is about advocating a disciplined, principled approach to visualization using modern tools (R / tidyverse / ggplot2), grounded in human perception, reproducibility, and iterated design.\n\n\nMain Points / Themes of the Talk\n“Think about the data first”\nBefore choosing visuals, deeply understand your data: its structure, variables, noise, limitations. The visualization should be driven by the nature of the data and the question.\n\nDecide on a message or story\n\nA visualization is not just to show “everything”. it should focus on what you want the audience to see and take away. The narrative behind the data helps guide the design choices.\n\nClean, structure, and preprocess the data\n\nGood visualizations depend on good data. Problems like missing values, inconsistent formats, or badly structured tables lead to misleading graphics.\n\nUse reproducible code / document your pipeline\n\nVisualizations should be produced in code to allow reproducibility, transparency, and easy modifications. Code should be readable, reproducible text.\n\n\nCommentary & Critique\nWickham’s principles resonate strongly with good practices in modern data science. The emphasis on reproducibility, code-based graphics, and principled design is very timely in an era when many visualizations are made in point-and-click tools (with little transparency). Wickham’s keynote is more of a philosophical and methodological guide than a technical demo showcase. It reinforces the idea that good data visualization is a discipline combining data wrangling, programming, perception science, and storytelling. For practitioners, it is a useful reminder of what good practice looks like and what trade-offs to consider."
  },
  {
    "objectID": "EPPS6302.html",
    "href": "EPPS6302.html",
    "title": "EPPS6302",
    "section": "",
    "text": "Methods of Data Collection and Production\n\nAssignment 2:\n\nR-package Google trends data vs. downloaded data on Google trends.\nDifferences: Downloaded Google trends data is easier for a quick look, and gives daily or weekly intervals. It is less adjustable with the time frame. “gtrendsR” package allows for more in-depth parameters of the search with time intervals and interest over time. The package allows for in-depth research of the data and data manipulation.\n\n\nAssignment 3:\nText Analysis\n\nAnalyze Biden-Xi Summit data: The key terms in the topic are china, america, Biden, Xijinping suggesting a focus on China-U.S. relations. Fentanyl and Corona-virus were also key health issues during this discussion surrounding China and U.S. trade. There is a cluster of topics surrounding human rights, uyghurgenocide, Tibetans and uyghurs. This cluster divides from Biden but contains central issues surrounding Tibetans and Xi for human rights. Taiwan, USA, US and CCP are peripheral nodes. Taiwan comes from China giving relevance to U.S.-China relations. CPP divides from Xijinping, given its location, it means the topic is not a dominant topic surrounding the US-China discussion.\n\nAnalyze U.S. presidential inaugural speeches: The use of “American” throughout the speeches is a fairly common used word but have varying distributions. Clinton, Trump and Biden have dense clusters of the usage of the word towards the end of their speech. This could be in part to address a focus on national rhetoric. The use of the word “Citizens” is mostly used as an address to people in the beginning of speeches indicated by the first mark towards the beginning of most of the speeches. In earlier speeches, the use of the word is less prominent, showing a shift in addressing people as active participants in government. The use of “freedom” is used fairly frequently. Bush and Reagan show heavy usage, indicating their political ideologies and focus on liberty during their presidency. In more recent speeches, Obama, Trump and Biden, use the word less frequently, showing a shift in the political focus.\nWhat is Wordfish? Wordfish is a Poisson scaling model of one-dimensional document positions. It allows for scaling documents without the need for reference scores/texts and uses unsupervised text scaling methods. It estimates the positions of documents based on observed word frequencies.\n\n\nAssignment 4:\nWeb Scraping\nUsing rvest to scrape Wikipedia page of foreign reserve data. This is how I modified the code to scrape the second table in the wiki page.\n#scrape second table assignment 4\n\nurl &lt;- 'https://en.wikipedia.org/wiki/List_of_countries_by_foreign-exchange_reserves'\n#Reading the HTML code from the Wiki website\nwikiforreserve &lt;- read_html(url)\nclass(wikiforreserve)\n\ncurrencyex &lt;- wikiforreserve %&gt;%\n  html_nodes(xpath='//*[@id=\"mw-content-text\"]/div[1]/table[2]') %&gt;%\n  html_table()\nclass(currencyex) # Why the first column is not scrapped?\ncurre &lt;- currencyex [[1]][,c(1, 2,3,4,5,6,7,8,9,10,11,12)]\nnames(curre) &lt;- c(\"Quarter\", \"USD\", \"EUR\", \"JPY\",\"GBP\", \"CAD\",\"RMB\", \"AUD\", \"CHF\", \"OtherCurrencies\", \"UnallocatedReserves\", \"Total\")\nwrite.csv(curre, \"currencycomp.csv\", row.names = FALSE)\nThe scraped data has some data inconsistency with how it is pulled from the URL. The table column names has be renamed or sometimes re-done to correct class. With the downloading from GovInfo, there has been delays with how you pull to avoid being flagged for suspicion. This makes the process slower, especially when downloading large data sets. With the Wikipedia data, the best way to improve is improving data type conversation.\n\n\nAssignment 5:\nYouTube API data on CNN channel\n\nThis a word cloud of the most commented words on CNN’s YouTube videos. There is frequent uses of polarized words such as “joke” and “garbage” indicating negative opinions regarding the topics. There are mentions of “Trump”, “Harris” and “Biden” showing prominent discussion on these political figures during the election, specifically with the words, “vote” and “president”. There is strong viewer engagement and emotional responses to CNN’s content.\n\nThis shows CNN’s video likes. The Video ID’s are provided but not the title so it is harder to infer specific topics and their engagement to audiences. However, we can still tell there is engagement in their videos through their likes and the audience stays interested. The high like count suggests that the content resonates with the viewers and may align with the viewer’s beliefs. Given the polarized comments, some videos may show public interest on the covered topics.\n\nThe peaks show an increased of published videos during the time frame. October 28 has a peak in publishing which might correlate with breaking news or significant political events. Lower publishing numbers can reflect a period with fewer newsworthy events or when CNN focuses on fewer in-depth reports. With the spikes, the higher viewer demand during news cycle. CNN increases their content output to cover stories or maintain viewer interest, which can correlate with publishing frequency.\n\n\nFinal Project:\nFinal Project\nFinal Presentation\n\n\nResources:\nhttps://rvest.tidyverse.org/ : scraping web data through R\nhttps://www.tidytextmining.com/ : text mining with R\nhttps://tutorials.quanteda.io/introduction/ : text analysis with R, Text data\nhttps://egap.org/methods-guides/"
  },
  {
    "objectID": "EPPS6356.html",
    "href": "EPPS6356.html",
    "title": "EPPS6356",
    "section": "",
    "text": "This site is used for my Data Visualization Class (EPPS6356).\nAshleigh’s Data Guide A personalized checklist for handling messy data."
  },
  {
    "objectID": "EPPS6356.html#assignment-1",
    "href": "EPPS6356.html#assignment-1",
    "title": "EPPS6356",
    "section": "Assignment 1",
    "text": "Assignment 1\n\nAnscombe’s (1973) paper highlights how datasets with nearly identical summary statistics (mean, variance, correlation, regression line) can nonetheless reveal strikingly different patterns when graphed. The four models demonstrate reliance on numerical measures alone can obscure key features such as non-linearity, outliers, and unusual data structures. The main lesson is the essential role of visualization in statistical analysis. Visualization of the models or data can improve quantitative summaries. A practical solution is to incorporate exploratory data analysis (EDA) techniques, such as scatterplots, boxplots, and residual plots, early in research to detect anomalies before modeling. Additionally, modern extensions like interactive visualizations and diagnostic checks can further safeguard against misleading interpretations, ensuring that conclusions rest on both robust statistics and an accurate understanding of the underlying data.\n\n\nThis chart from a UNICEF–Gallup survey shows what young people (ages 15–24) in 21 countries believe is the most important factor in determining success. The side-by-side horizontal bar charts are helpful for seeing within-country differences, but the design makes cross-country comparisons harder than necessary. Because the categories are split into four separate panels, the viewer has to scan back and forth across columns rather than directly comparing values on a single axis. The sorting of countries by income per capita adds context, but introduces a visual bias. Readers may infer causal relationships between national wealth and values without the chart explicitly addressing them. A clearer alternative would be to combine responses into a grouped or stacked bar chart, aligning all categories for each country in one view. Small multiples with consistent scales would make it easier to compare how emphasis on education, hard work, or family wealth varies across regions. Simplifying the layout would improve readability and strengthen the chart’s ability to communicate cultural differences in beliefs about success."
  },
  {
    "objectID": "EPPS6356.html#assignment-2",
    "href": "EPPS6356.html#assignment-2",
    "title": "EPPS6356",
    "section": "Assignment 2",
    "text": "Assignment 2\nPaul Murrell’s RGraphics basic R programs using Happy Planet Index Data set (2024)\n\nlibrary(readxl)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nhpi = read_excel(\"hpi_2024_public_dataset.xlsx\", sheet = \"1. All countries\", skip = 7)\n\nNew names:\n• `` -&gt; `...4`\n\nhead(hpi)\n\n# A tibble: 6 × 12\n  `HPI rank` Country     ISO   ...4    Continent `Population (thousands)`\n       &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;                    &lt;dbl&gt;\n1         NA Burundi     BDI   2021BDI         5                   12551.\n2         NA Sudan       SDN   2021SDN         5                   45657.\n3          1 Vanuatu     VUT   2021VUT         8                     319.\n4          2 Sweden      SWE   2021SWE         3                   10467.\n5          3 El Salvador SLV   2021SLV         1                    6314.\n6          4 Costa Rica  CRI   2021CRI         1                    5154.\n# ℹ 6 more variables: `Life Expectancy (years)` &lt;dbl&gt;,\n#   `Ladder of life (Wellbeing) (0-10)` &lt;dbl&gt;,\n#   `Carbon Footprint (tCO2e)` &lt;dbl&gt;, HPI &lt;dbl&gt;,\n#   `CO2 threshold for year  (tCO2e)` &lt;dbl&gt;, `GDP per capita ($)` &lt;dbl&gt;\n\ncolnames(hpi)\n\n [1] \"HPI rank\"                          \"Country\"                          \n [3] \"ISO\"                               \"...4\"                             \n [5] \"Continent\"                         \"Population (thousands)\"           \n [7] \"Life Expectancy (years)\"           \"Ladder of life (Wellbeing) (0-10)\"\n [9] \"Carbon Footprint (tCO2e)\"          \"HPI\"                              \n[11] \"CO2 threshold for year  (tCO2e)\"   \"GDP per capita ($)\"               \n\nplot(hpi$\"Life Expectancy (years)\", hpi$HPI, pch = 16, col = \"darkblue\",\n     xlab = \"Life Expectancy (years)\",\n     ylab = \"Happy Planet Index (HPI)\")\n\n\n\n\n\n\n\nhist(hpi$\"Life Expectancy (years)\", breaks = 20, col = \"lightblue\",\n     border = \"white\",\n     main = \"Histogram of Life Expectancy\",\n     xlab = \"Life Expectancy (years)\")\n\n\n\n\n\n\n\nboxplot(hpi$HPI, horizontal = TRUE, col = \"lightgreen\",\n        main = \"Boxplot of Happy Planet Index (HPI)\",\n        xlab = \"Happy Planet Index (HPI)\")\n\n\n\n\n\n\n\nhpi &lt;- hpi %&gt;%\n  rename(LifeExp = `Life Expectancy (years)`)\n\n# ---- 1. Scatter with par(), points(), lines(), axis(), box(), text(), mtext(), legend() ----\npar(mfrow = c(2, 2), mar = c(4, 4, 2, 1)) # grid layout\n\nplot(hpi$LifeExp, hpi$HPI,\n     xlab = \"Life Expectancy (years)\", ylab = \"HPI\",\n     main = \"Life Expectancy vs. HPI\", col = \"blue\", pch = 16)\n\nabline(lm(HPI ~ LifeExp, data = hpi), col = \"red\", lwd = 2)\n# Add top axis\naxis(3, at = seq(40, 85, 10), labels = paste0(seq(40, 85, 10), \" yrs\"))\n\n# Add box around plot\nbox(lwd = 2)\n\n# Add annotation\ntext(65, 45, \"Higher LifeExp = Higher HPI\", col = \"purple\")\n\n# Add margin text\nmtext(\"Source: HPI 2024\", side = 1, line = 2, col = \"gray40\")\n\n# Add legend\nlegend(\"bottomright\", legend = c(\"Countries\", \"Regression line\"),\n       col = c(\"blue\", \"red\"), pch = c(16, NA), lty = c(NA, 1), bty = \"n\")\n\n# ---- 2. Histogram ----\nhist(hpi$HPI, col = \"lightblue\", border = \"darkblue\",\n     main = \"Distribution of HPI\", xlab = \"HPI\")\n\n# ---- 3. Boxplot ----\nboxplot(HPI ~ Continent, data = hpi,\n        col = \"tan\", main = \"HPI by Continent\",\n        xlab = \"Continent\", ylab = \"HPI\")\n\n# ---- 4. Pie chart + names() ----\n\ncontinent_labels &lt;- c(\n  \"1\" = \"Latin America & Caribbean\",\n  \"2\" = \"N. America & Oceania\",\n  \"3\" = \"Western Europe\",\n  \"4\" = \"Middle East\",\n  \"5\" = \"Africa\",\n  \"6\" = \"South Asia\",\n  \"7\" = \"Eastern Europe & Central Asia\",\n  \"8\" = \"East Asia\"\n)\n\n# Replace numeric codes with names\nhpi$Continent &lt;- as.character(hpi$Continent)\nhpi$Continent &lt;- continent_labels[hpi$Continent]\n#hpi$Continent &lt;- as.character(hpi$Continent)\n#hpi$Continent &lt;- ifelse(hpi$Continent %in% names(continent_labels),\n                        #continent_labels[hpi$Continent],\n                        #\"Unknown\")\n\ncontinent_counts &lt;- table(hpi$Continent)\n\npie(continent_counts,\n    col = rainbow(length(continent_counts)),\n    main = \"Countries by Continent\")\n\n\n\n\n\n\n\n# ---- 5. Perspective plot ----\n# Create grid for LifeExp vs. Carbon → predict HPI\n# Remove non-numeric characters and convert\nhpi$Carbon &lt;- as.numeric(hpi$`Carbon Footprint (tCO2e)`)\n\n# Confirm it worked\nsummary(hpi$Carbon)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.201   2.182   5.488   7.305  10.107  42.201 \n\nx_seq &lt;- seq(min(hpi$LifeExp, na.rm=TRUE), max(hpi$LifeExp, na.rm=TRUE), length=30)\ny_seq &lt;- seq(min(hpi$Carbon, na.rm=TRUE), max(hpi$Carbon, na.rm=TRUE), length=30)\n\nz_mat &lt;- outer(x_seq, y_seq, function(a, b) {\n  predict(lm(HPI ~ LifeExp + Carbon, data=hpi),\n          newdata=data.frame(LifeExp=a, Carbon=b))\n})\n\npersp(x_seq, y_seq, z_mat,\n      theta = 30, phi = 20, expand = 0.6, col = \"lightgreen\",\n      xlab = \"Life Expectancy\", ylab = \"Carbon Footprint\", zlab = \"Predicted HPI\",\n      main = \"3D Perspective Plot\")"
  },
  {
    "objectID": "EPPS6356.html#class-competition-for-assignment-2",
    "href": "EPPS6356.html#class-competition-for-assignment-2",
    "title": "EPPS6356",
    "section": "Class Competition for assignment 2",
    "text": "Class Competition for assignment 2\n\nlibrary(readxl)\nhpi = read_excel(\"hpi_2024_public_dataset.xlsx\", sheet = \"1. All countries\", skip = 7)\n\nNew names:\n• `` -&gt; `...4`\n\nboxplot(HPI ~ Continent, data = hpi, col = \"tan\", main = \"HPI by Continent\", xlab = \"Continent\", ylab = \"HPI\")"
  },
  {
    "objectID": "EPPS6356.html#assignment-3",
    "href": "EPPS6356.html#assignment-3",
    "title": "EPPS6356",
    "section": "Assignment 3",
    "text": "Assignment 3\n\n# Barplot Murrell's graphics\npar(mar=c(2, 3.1, 2, 2.1)) #adjusts plot margins. bottom=2, left=3.1, top=2, right=2.1\nmidpts &lt;- barplot(VADeaths, \n                  col=gray(0.1 + seq(1, 9, 2)/11), \n                  names=rep(\"\", 4)) #col sets a gray scale to the points\n\n#mtext adds text to the margins of the plot. adds custom x-axis labels beneath bars\n# side=1 draw at the bottom of the plot ,line=0.5 moves labels closer to the bars\n# cex=0.5 makes the font smaller\nmtext(sub(\" \", \"\\n\", colnames(VADeaths)),\n      at=midpts, side=1, line=0.5, cex=0.5)\n\n#rep adds a bar at each midpoint for each group, adding cumulative sums of the deaths\ntext(rep(midpts, each=5), apply(VADeaths, 2, cumsum) - VADeaths/2,\n     VADeaths, \n     col=rep(c(\"white\", \"black\"), times=3:2), \n     cex=0.8) #shrinks text slightly \n\n\n\n\n\n\n\n# Reset margins back to default\npar(mar=c(5.1, 4.1, 4.1, 2.1))  \n\n\n# Anscombe Visulization with RGraphics\ndata(anscombe)  # Load Anscombe's data\nView(anscombe)\n\nop &lt;- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma = c(0, 0, 2, 0), family=\"serif\")\nff &lt;- y ~ x\nmods &lt;- setNames(as.list(1:4), paste0(\"lm\", 1:4))\n\n# Define colors and point characters\ncols &lt;- c(\"tomato\", \"royalblue\", \"forestgreen\", \"orange\")\npch_vals &lt;- c(19, 17, 15, 8) \n\n# Loop over 4 datasets\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  mods[[i]] &lt;- lmi &lt;- lm(ff, data = anscombe)\n  \n  plot(ff, data = anscombe,\n       col = cols[i], pch = pch_vals[i], cex = 1.3,\n       xlim = c(3, 19), ylim = c(3, 13),\n       main = paste(\"Dataset\", i))\n  \n  abline(lmi, col = \"black\", lty = 2, lwd = 2)\n}\n\nmtext(\"Anscombe's 4 Regression Data Sets (Customized)\", outer = TRUE, cex = 1.5)\n\n\n\n\n\n\n\npar(op)\n\n#Anscombe Visualization with ggplot2\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.5.2\n\n\nWarning: package 'readr' was built under R version 4.5.2\n\n\nWarning: package 'purrr' was built under R version 4.5.2\n\n\nWarning: package 'stringr' was built under R version 4.5.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.1     ✔ readr     2.1.6\n✔ ggplot2   4.0.1     ✔ stringr   1.6.0\n✔ lubridate 1.9.4     ✔ tibble    3.3.0\n✔ purrr     1.2.0     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndata(anscombe)  # Load Anscombe's data\nanscombe_long &lt;- anscombe %&gt;%\n  pivot_longer(\n    cols = everything(),\n    names_to = c(\".value\", \"set\"),\n    names_pattern = \"(.)(.)\"\n  ) %&gt;%\n  mutate(set = factor(set, labels = paste(\"Dataset\", 1:4)))\nView(anscombe_long)\nggplot(anscombe_long, aes(x = x, y = y)) +\n  geom_point(aes(color = set, shape = set), size = 3) +      # different colors + shapes\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\", linetype = \"dashed\") + # regression line\n  facet_wrap(~ set) +                                        # one plot per dataset\n  theme_bw(base_family = \"serif\") +                          # serif font\n  labs(title = \"Anscombe's Quartet with ggplot2\",\n       subtitle = \"Same regression models, very different data\") +\n  theme(legend.position = \"none\") +  \n  scale_shape_manual(values = c(16, 17, 15, 18))  # dataset1=circle, dataset2=triangle, dataset3=square, dataset4=solid diamond\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "EPPS6356.html#assignment-4-hackathon",
    "href": "EPPS6356.html#assignment-4-hackathon",
    "title": "EPPS6356",
    "section": "Assignment 4 (Hackathon)",
    "text": "Assignment 4 (Hackathon)\n\ndata(iris)\n\nlibrary(tidyverse)\n# 1. Divide the dataset into three rectangles based on species.\n# The average of Petal.Length and Petal.Width is the length and width.\n# Draw three rectangles arranged horizontally.\n\n#1\n\nplot_data &lt;- iris %&gt;%\n  mutate(\n    sepal_length_group = cut(\n      Sepal.Length,\n      breaks = c(4, 5.5, 7.0, 8.0),\n      labels = c(\"Small (4.0-5.5)\", \"Medium (5.6-7.0)\", \"Large (7.1-8.0)\"),\n      include.lowest = TRUE\n    )\n  ) %&gt;%\n  group_by(sepal_length_group) %&gt;%\n  summarise(\n    count = n(),\n    avg_petal_length = mean(Petal.Length)\n  ) %&gt;%\n  mutate(\n    xmax = cumsum(count),\n    xmin = xmax - count,\n    x_label_pos = (xmin + xmax) / 2\n  )\n\nggplot(plot_data, aes(ymin = 0)) +\n  geom_rect(\n    aes(\n      xmin = xmin,\n      xmax = xmax,\n      ymax = avg_petal_length,\n      fill = sepal_length_group\n    ),\n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    breaks = plot_data$x_label_pos,\n    labels = plot_data$count\n  ) +\n  scale_fill_brewer(palette = \"viridis\", direction = -1) +\n  labs(\n    title = \"Average Petal Length by Sepal Length Group\",\n    subtitle = \"Column width is proportional to the number of flowers in each group\",\n    x = \"Count of Flowers in Group\",\n    y = \"Average Petal Length (cm)\",\n    fill = \"Sepal Length Group\"\n  ) +\n  # Apply a clean theme\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 18),\n    legend.position = \"bottom\",\n    panel.grid.major.x = element_blank(), # Remove vertical grid lines\n    panel.grid.minor.x = element_blank()\n  )\n\nWarning: Unknown palette: \"viridis\"\n\n\n\n\n\n\n\n\n# 2. table with embedded charts\niris_long &lt;- iris %&gt;%\n  pivot_longer(cols = -Species, names_to = \"Measurement\", values_to = \"Value\")\n\nggplot(iris_long, aes(x = Value, fill = Species)) +\n  geom_histogram(color = \"white\", bins = 15) +\n  facet_grid(Species ~ Measurement, scales = \"free\") +\n  scale_fill_manual( #coloring each species\n    values = c(\n      \"setosa\" = \"steelblue\", \n      \"versicolor\" = \"orange\",   \n      \"virginica\" = \"seagreen\"     \n    ) \n    ) + #labels\n      labs(\n        title = \"Distribution of Iris Measurements by Species\",\n        x = \"Measurement Value (cm)\",\n        y = \"Count\"\n      ) +\n  theme_bw() +\n    theme(\n      plot.title = element_text(hjust = 0.5, face = \"bold\"),\n      strip.text.x = element_text(face = \"bold\"),\n      strip.text.y = element_text(face = \"bold\"),\n      panel.border = element_rect(color = \"grey80\", fill = NA),\n      legend.position = \"bottom\"\n    )\n\n\n\n\n\n\n\n# 3. Extract setona and versicolor from species.\n# Then create df_2 and df_3. Draw a bar plot using petal.width: p1 p2.\n# Finally, use gridExtra to combine the plots.'\nlibrary(\"gridExtra\")\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\ndf_2 &lt;- subset(iris, Species %in% \"setosa\")\ndf_3 &lt;- subset(iris, Species %in% \"versicolor\")\ndf_2$id &lt;- 1:nrow(df_2)\ndf_3$id &lt;- 1:nrow(df_3)\n\n\n\np1 = ggplot(df_2, aes(x = factor(id), y = Petal.Width)) +\n  geom_bar(stat = \"identity\", fill = 'red', color = \"black\") +\n  coord_flip() +\n  labs(title = \"setosa\") +\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank() #this was by GPT\n  )\n\n\np2 = ggplot(df_3, aes(x = factor(id), y = Petal.Width)) +\n  geom_bar(stat = \"identity\", fill = \"blue\", color = \"black\") +\n  coord_flip() +\n  labs(title = \"versicolor\")+\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank() #this was by GPT\n  )\n\n\n\ngridExtra::grid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n# 4 Column Chart\n# getting means of Petal length and width for each species\n# and mean sepal length and sepal width\niris_means &lt;- iris %&gt;%\n  group_by(Species) %&gt;%\n  summarise(\n    mean_sepal_length = mean(Sepal.Length),\n    mean_sepal_width = mean(Sepal.Width),\n    mean_petal_length = mean(Petal.Length),\n    mean_petal_width = mean(Petal.Width)\n  ) %&gt;%\n  pivot_longer(\n    cols = -Species,\n    names_to = \"Measurement\",\n    values_to = \"MeanValue\"\n  )\n\nggplot(iris_means, aes(x = Measurement, y = MeanValue, fill = Species)) +\n  geom_col(position = position_dodge(width = 0.8)) + \n  labs(title = \"Mean Iris Measurements by Species\",\n       x = \"Measurement\", y = \"Mean Value\") + \n  theme_minimal(base_size = 12) +\n  scale_fill_manual(values = c(\"steelblue\", \"orange\", \"seagreen\"))"
  },
  {
    "objectID": "EPPS6356.html#class-coding-competition",
    "href": "EPPS6356.html#class-coding-competition",
    "title": "EPPS6356",
    "section": "Class coding competition",
    "text": "Class coding competition\n\nlibrary(ggplot2)\nmpg &lt;- as.data.frame(mpg)\n#2seater, compact, midsize, minivan, pickup, subcompact, suv scatterplots in one view\nggplot(mpg, aes(x=displ, y=hwy)) +\n  geom_point(color = \"black\") +\n  facet_wrap(~ class) +\n  labs(x=\"displ\",\n       y=\"hwy\") +\n  theme_gray()\n\n\n\n\n\n\n\n#improving the chart\nggplot(mpg, aes(x=displ, y=hwy)) +\n  geom_point(color = \"blue\", size=2, alpha=0.3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#E65100\", linewidth = 0.8) +\n  facet_wrap(~ class) +\n  labs(title=\"Engine Displacement vs Highway MPG by Vehicle Class\",\n       x=\"Engine Displacement (liters)\",\n       y=\"Highway Miles per Gallon (MPG)\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size=16, face=\"bold\"),\n    axis.title.x = element_text(size=12),\n    axis.title.y = element_text(size=12)\n  )\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "EPPS6356.html#assignment-5",
    "href": "EPPS6356.html#assignment-5",
    "title": "EPPS6356",
    "section": "Assignment 5",
    "text": "Assignment 5\n\ndata(iris)\ncolors &lt;- c(\"lightpink\", \"lightgreen\", \"lightblue\")\npch_shapes &lt;- c(16, 17, 18)[as.factor(iris$Species)]\nfamily &lt;- \"serif\"\n\n#Histogram\nhist(iris$Sepal.Length,\n     col = \"lightblue\",\n     border = \"white\",\n     main = \"Histogram of Sepal Length\",\n     xlab = \"Sepal Length (cm)\",\n     ylab = \"Frequency\",\n     breaks = 10,\n     family = \"serif\")\n\n\n\n\n\n\n\n#Barchart Vertical\nbarplot(table(iris$Species),\n        col = colors,\n        main = \"Bar Chart of Iris Species\",\n        xlab = \"Species\",\n        ylab = \"Count\",\n        family = family)\n\n\n\n\n\n\n\n#Barchart Horizontal\nbarplot(table(iris$Species),\n        col = colors,\n        main = \"Bar Chart of Iris Species\",\n        xlab = \"Count\",\n        ylab = \"Species\",\n        horiz = TRUE,\n        family = family)\n\n\n\n\n\n\n\n#PieChart\npie(table(iris$Species),\n    col = colors,\n    main = \"Pie Chart of Iris Species\",\n    family = family)\n\n\n\n\n\n\n\n#Boxplot\nboxplot(Sepal.Length ~ Species,\n        data = iris,\n        col = colors,\n        main = \"Boxplot of Sepal Length by Species\",\n        xlab = \"Species\",\n        ylab = \"Sepal Length (cm)\",\n        family = family)\n\n\n\n\n\n\n\n#Scatterplot\nplot(iris$Sepal.Length, iris$Sepal.Width,\n     col = colors[as.factor(iris$Species)],\n     pch = pch_shapes,\n     main = \"Scatter Plot of Sepal Length vs Sepal Width\",\n     xlab = \"Sepal Length (cm)\",\n     ylab = \"Sepal Width (cm)\",\n     family = family)\n\n\n\n\n\n\n\n#ggplot2\nlibrary(ggplot2)\n#Histogram\nggplot(iris, aes(x = Sepal.Length)) +\n  geom_histogram(binwidth = 0.5, fill = \"lightblue\", color = \"white\") +\n  labs(title = \"Histogram of Sepal Length\",\n       x = \"Sepal Length (cm)\",\n       y = \"Frequency\") +\n  theme(text = element_text(family = family))\n\n\n\n\n\n\n\n#Barchart Vertical\nggplot(iris, aes(x = Species, fill = Species)) +\n  geom_bar() +\n  scale_fill_manual(values = colors) +\n  labs(title = \"Bar Chart of Iris Species\",\n       x = \"Species\",\n       y = \"Count\") +\n  theme(text = element_text(family = family))\n\n\n\n\n\n\n\n#Barchart Horizontal\nggplot(iris, aes(x = Species, fill = Species)) +\n  geom_bar() +\n  scale_fill_manual(values = colors) +\n  coord_flip() +\n  labs(title = \"Bar Chart of Iris Species\",\n       x = \"Species\",\n       y = \"Count\") +\n  theme(text = element_text(family = family))\n\n\n\n\n\n\n\n#PieChart\nggplot(iris, aes(x = \"\", fill = Species)) +\n  geom_bar(width = 1) +\n  coord_polar(theta = \"y\") +\n  scale_fill_manual(values = colors) +\n  labs(title = \"Pie Chart of Iris Species\") +\n  theme_void() +\n  theme(text = element_text(family = family))\n\n\n\n\n\n\n\n#Boxplot\nggplot(iris, aes(x = Species, y = Sepal.Length, fill = Species)) +\n  geom_boxplot() +\n  scale_fill_manual(values = colors) +\n  labs(title = \"Boxplot of Sepal Length by Species\",\n       x = \"Species\",\n       y = \"Sepal Length (cm)\") +\n  theme(text = element_text(family = family))\n\n\n\n\n\n\n\n#Scatterplot\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species, shape = Species)) +\n  geom_point(size = 3) +\n  scale_color_manual(values = colors) +\n  labs(title = \"Scatter Plot of Sepal Length vs Sepal Width\",\n       x = \"Sepal Length (cm)\",\n       y = \"Sepal Width (cm)\") +\n  theme(text = element_text(family = family))\n\n\n\n\n\n\n\n# Base R example, saving\n# PDF\n# pdf(\"histogram_iris.pdf\")\n# hist(iris$Sepal.Length,\n#      col = \"lightblue\",\n#      border = \"white\",\n#      main = \"Histogram of Sepal Length\",\n#      xlab = \"Sepal Length (cm)\",\n#      ylab = \"Frequency\",\n#      breaks = 10,\n#      family = \"serif\")\n# dev.off()\n\n# JPG\n# jpeg(\"bar_vertical_iris.jpg\", width=800, height=600)\n# barplot(table(iris$Species),\n#         col = colors,\n#         main = \"Bar Chart of Iris Species\",\n#         xlab = \"Species\",\n#         ylab = \"Count\",\n#         family = family)\n# dev.off()\n\n# SVG\n# svg(\"barchart_horizontal_iris.svg\")\n# barplot(table(iris$Species),\n#        col = colors,\n#        main = \"Bar Chart of Iris Species\",\n#        xlab = \"Count\",\n#        ylab = \"Species\",\n#        horiz = TRUE,\n#        family = family)\n# dev.off()\n\n# TIFF\n# tiff(\"piechart_iris.tiff\", width=800, height=600)\n# pie(table(iris$Species),\n#     col = colors,\n#     main = \"Pie Chart of Iris Species\",\n#     family = family)\n# dev.off()\n\n# BMP\n# bmp(\"boxplot_iris.bmp\", width=800, height=600)\n# boxplot(Sepal.Length ~ Species,\n#         data = iris,\n#         col = colors,\n#         main = \"Boxplot of Sepal Length by Species\",\n#         xlab = \"Species\",\n#         ylab = \"Sepal Length (cm)\",\n#         family = family)\n# dev.off()\n\n# PDF/SVG: Vector graphics (scalable, ideal for publications).\n#JPG/BMP/TIFF: Raster graphics (pixel-based, may lose quality when resized).\n# TIFF: High-quality raster, often used in print.\n# BMP: Simple raster format, large file size, rarely used."
  },
  {
    "objectID": "EPPS6356.html#assignment-6",
    "href": "EPPS6356.html#assignment-6",
    "title": "EPPS6356",
    "section": "Assignment 6",
    "text": "Assignment 6\na. custom ui:\n\n# ui &lt;- fluidPage(\n#   # --- Custom Styles ---\n#   tags$style(HTML(\"\n#     body {\n#       background-color: white;\n#       color: black;\n#     }\n#     h1 {\n#       font-family: 'Palatino', sans-serif;\n#     }\n#     .shiny-input-container {\n#       color: #1E90FF;\n#     }\n#   \")),\n\nb/c: shiny app with mtcars, USArrests, and uspop\n\n\n\n\nd/e: shiny app with my own data"
  },
  {
    "objectID": "EPPS6356.html#assignment-7",
    "href": "EPPS6356.html#assignment-7",
    "title": "EPPS6356",
    "section": "Assignment 7",
    "text": "Assignment 7"
  },
  {
    "objectID": "IngeDruckrey.html",
    "href": "IngeDruckrey.html",
    "title": "Review of Inge Druckrey’s Teaching to see",
    "section": "",
    "text": "Inge Druckrey, a celebrated educator and graphic designer, often teaches students to see beyond literal shapes and into the subtleties of form, particularly the interplay of thick and thin strokes, and curves versus straight lines. Thin and thick strokes can be aesthetic choices and they help guide the eye, create rhythm, contrast, and convey structure. In type and design, varying stroke weight defines hierarchy and visual tension. Curves vs. linearity help communicate balance and movement. Curves suggest organic flow and softness, while straight or angular lines evoke structure and precision. Druckrey encourages observing these contrasts in everyday visual culture highlighting how they deeply affect perception. Designers like Holmes often underscore: The eye is the observer: it perceives form, nuance, proportion, and subtle visual qualities. The hand is the creator: it responds to what the eye registers, translating intent into form through gesture, weight, and rhythm. In writing or calligraphy, the hand modifies what the eye envisions, correcting and balancing shapes through tactile feedback. The eye seeks harmony and optical accuracy, while the hand expresses through pressure, flow, and human imperfection. Steve Jobs often cited a calligraphy class as transformational in shaping his design philosophy. He learned about serif vs. sans-serif, spacing between letter pairs, and what makes typography great. These skills that seemed impractical at the time but later profoundly influenced the Macintosh’s design. In his 2005 Stanford commencement address, Jobs explained how this appreciation for beautiful, historical, and artistically subtle typography was integrated into the Mac. Making it the first personal computer with multiple typefaces and proportionally spaced fonts. His calligraphy experience shaped Apple’s commitment to marrying technology with liberal arts, embedding beauty into utility. Moreover, Hermann Zapf’s typeface Zapfino was later included in Apple’s OS X, further echoing this calligraphic sensibility.\nGeometric accuracy is an exact, mathematically precise rendering of shapes, such as perfect circles, straight lines, consistent angles. Optical accuracy is adjusting forms to appear visually correct to the human eye, compensating for perceptual biases. Examples include: Overshoot: extending round shapes slightly above their height to appear equal to straight-edged forms. Weight distribution: on small screens, curves may require thicker strokes to avoid appearing too light or sparse. Designers like Susan Kare (icon and font designer for the original Macintosh) practiced optical accuracy. She used pixel grids to make fonts and icons look perfectly readable and balanced, even if they weren’t mathematically uniform."
  },
  {
    "objectID": "Other.html",
    "href": "Other.html",
    "title": "Other",
    "section": "",
    "text": "This page will consist of conferences I’ve attended or other professional developments.\n\nConferences and Badges\n\nOctober 5th - 6th 2024: Rowdy Datathon at UTSA\n\nDuring this datathon, I learned more methods of data visualization using Python and pandas in python, time series and analysis and leveraging SQL with Python for data analysis.\nI also learned about large language models (LLM): Interacting LLMs can be modeled with traditional population dynamics approaches. LLMs have to be used with caution due to the phenomenon of invalidation (a.k.a. hallucination). Consensus approaches  can minimize (or eliminate) invalidation of information. The use of consensus can yield a potent tool for research.\n\n\nPublic and Non-Profit Managment Conference - April 25th - 2025\nI attended the 9th Annual PNM conference in innovation in public and nonprofit management. Learning from a panel to understand technological and social innovation in human-centric workplace and cyber security events and risk.\n\n\nPathway to Professional Digital Badge Program - UTD\nThe Pathway to Professionalism Badge is designed to increase career readiness skills, specifically the National Association of College & Employers (NACE) Career Readiness Competencies. Career readiness skills can boost a student’s competitive advantage in the workforce, including the job search and the job retention processes. By gaining career readiness skills, students increase their confidence and ability to succeed in the world of work. Awarded on May 22, 2025.\n\n\nRESULTS Fellowship Conference - July 13th-15th 2025\nI had the opportunity to join emerging leading from across the country as we came together to build our advocacy skills, connect over shared goals, and prepare for a powerful day of action on Capital Hill. Together, we met with members of Congress to advocate for policies that address poverty, expand access to healthcare, and promote equity. Grateful to be a part of this movement for lasting change!\n\n\n\nDallas RESULTS Fundraiser - October 23rd 2025\nI was honored to take part in the RESULTS Dallas Fundraiser on Thursday, October 23, where I spoke about my journey as a RESULTS Fellow. It was inspiring to hear from Tenzin Kunor, RESULTS’ Senior Associate for Global Policy, whose advocacy and lived experience as a TB survivor highlighted the real-world impact of equitable health policies. Through the Fellowship, I’ve had the opportunity to connect grassroots voices with public policy, amplify marginalized perspectives, and contribute to efforts that uplift communities around the world. During this experience, I’ve learned that: Advocacy has real power: everyday people can influence policies that address poverty and inequality. Storytelling matters: sharing lived experiences helps make policy recommendations real and compelling. Community builds strength: being part of a movement working for justice is truly life-changing. Thank you to RESULTS and RESULTS Educational Fund for this platform of possibility and to everyone who believes change isn’t just possible, but inevitable when we engage.\n\n\nNorth Texas Quality of Life Initiative - October 24th 2025\nOur team shared our latest project update for the North Texas Quality of Life Initiative! We presented progress on the Quality of Life Dashboard 1.5, which incorporates 2024 and 2025 data to better capture changes in education, social, economic, and environmental well-being across North Texas. We also gave a sneak peek of what’s coming in version 2.0, where I’m developing a raster map of school scores to visualize educational quality and spatial patterns across the region. Learn more about this initiative at: North Texas Quality of Life Initiative\n\n\n\nPositive Emotions Across Race - November 14th 2025\nI had the opportunity to present my project “Positive Emotions Across Race” at the University of Texas at Dallas. I analyzed North Texas Quality of Life survey data to understand how expressions of positive emotions vary across racial groups while controlling for key demographic factors such as age, gender, income, education, and employment status. Through logistic regression, I uncovered meaningful patterns in how communities experience and express positive emotions. Key findings: Asian and Black individuals consistently show higher odds of reporting positive emotions compared to White participants. Hispanic participants show higher positive emotions, but only after accounting for socioeconomic, education, and employment factors. These patterns highlight important differences in how communities experience positive emotions, even after considering demographic and socioeconomic factors. This work is part of my ongoing research in the Social Data Analytics & Research program at UT Dallas, where I’m focused on connecting statistical methods with real-world insights. I’m grateful for the chance to share this work, and I’m excited to continue growing as a data scientist, enhancing my skills in coding, data cleaning, analysis, and visualizing data to translate insights into meaningful, real-world impact.\n\n\n\n\n\n\n\nIn Publications\n\nMedicaid LTE\nIn Dallas Morning News: Friday May 16th, 2025.\nCuts will gut Medicaid\nRe: “Congress can’t cut Medicaid without cutting services — In Texas, recipients are pregnant women, children, those with disabilities and nursing home residents,” by Fred Cerise, Monday Opinion.\nAt a time when millions of Americans rely on Medicaid for essential care, Congress is debating billions in cuts that could devastate access, while downplaying the consequences.\nPolicymakers are proposing to slash Medicaid funding under the guise of eliminating fraud. But let’s be clear: You can’t gut the funding without gutting the services.\nIn Texas, that means fewer doctors, less care and real harm to people already struggling. One major target is a legal funding mechanism used by almost every state: provider taxes that help unlock federal Medicaid dollars. If Congress lowers the cap on these taxes, Texas could lose nearly $5 billion in care. That’s not fraud reduction; that’s cutting care.\nWe need leaders who will protect our most vulnerable, not quietly dismantle the system they rely on. Call your representative today and urge them to oppose these harmful Medicaid cuts. Texans can’t afford to lose the care they depend on.\nAshleigh M. Frank, North Dallas\n\n\nSNAP LTE\nIn Dallas Morning News: Sunday May 18th, 2025\nPrioritize food policies\nRe: “Texas leads nation in hunger — Rising prices for necessities outpace D-FW family budgets,” Thursday news story.\nAs a graduate student at the University of Texas at Dallas, I often plan my meals around the limited food I receive from the campus pantry, supported by the North Texas Food Bank. Despite working and budgeting, I can’t always afford enough food, and I know I’m not alone. According to UTD’s 2023 Your Needs Survey, 25.8% of students ate less than they should because they couldn’t afford to buy more food. But food insecurity goes beyond campus. Many families today are struggling with rising grocery costs and other essential expenses. No one should have to choose between basic needs. Hunger affects our communities, our workforce and our future. Congress has the opportunity to address this in the next Farm Bill by strengthening SNAP benefits and making them more accessible. We must prioritize policies that ensure every person, student, worker, parent or elder has reliable access to nutritious food. I urge readers to contact their congressional members to prioritize this urgent issue and protect the health and well-being of all Texans.\nAshleigh M. Frank, Dallas\n\n\nProtect Healthcare LTE\nIn Dallas Morning News on Monday, June 30th, 2025\nProtect Medicaid and hospitals\nCongress should not rush to pass a massive budget reconciliation bill just to meet a political deadline. Buried in the bill are deep Medicaid cuts, more extreme than what the House proposed, that would gut funding mechanisms states rely on to serve low-income residents. This threatens the viability of rural hospitals and community health centers, especially here in Texas. Medicaid is a lifeline for millions, and slashing it to meet unrealistic budget targets is irresponsible. Even some Republicans recognize the harm this bill could do. I urge Sens. John Cornyn and Ted Cruz to vote no on the bill. Protect Medicaid, protect our hospitals and protect Texas families.\nAshleigh M. Frank, North Dallas\n\n\nVoter Suppression LTE\nIn Dallas Morning News on Wednesday, August 6th, 2025.\nThis is voter suppression\nRe: “House panel passes GOP redistricting plan,” Sunday Metro story.\nAs a Texas voter, it’s frustrating to watch our elected officials redraw the lines of power in secret, while ignoring the voices of the very people they claim to represent. During public hearings, Texans overwhelmingly spoke out against the redistricting plan, yet lawmakers passed it anyway. Some even denied seeing maps until days before the vote. This isn’t redistricting. It’s voter suppression in plain sight. The current proposal weakens minority voting strength, benefits one party and undermines the foundations of our democracy. We need transparent, accountable leadership, not political maneuvering behind closed doors. I urge Texans to contact their representatives and demand a fair and inclusive process that reflects all of us, not just the powerful few.\nAshleigh M. Frank, North Dallas\n\n\nUnity LTE\nIn Dallas Morning News: Thursday October 2nd, 2025\nRemember this truth\nIn times of bitter division, the Constitution and a simple song can still remind us of who we are. We are becoming a nation defined by anger and partisanship, drifting away from the ideals of justice, liberty and community that once united us. As de Vinck’s essay reminds us, the Constitution provides the blueprint for shared governance. The lyrics of It’s a Small World carry a profound truth: That despite differences, we live under the same sun and moon. National renewal starts with remembering these truths. Let us recommit to the principles of our Constitution and embrace the humility to listen, respect and heal together. America’s strength has always come from unity in diversity.\nAshleigh M. Frank, Dallas"
  },
  {
    "objectID": "prehackathon.html",
    "href": "prehackathon.html",
    "title": "Pre-hackathon",
    "section": "",
    "text": "## Download COVID data from OWID GitHub\nowidall = read.csv(\"https://github.com/owid/covid-19-data/blob/master/public/data/owid-covid-data.csv?raw=true\")\n# Deselect cases/rows with OWID\nowidall = owidall[!grepl(\"^OWID\", owidall$iso_code), ]\n# Subset by continent: Europe\nowideu = subset(owidall, continent==\"Europe\")\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.5.2\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nowideu &lt;- owideu %&gt;%\n  mutate(date = as.Date(date, format = \"%Y-%m-%d\")) %&gt;%\n  filter(!is.na(new_deaths))\n\nggplot(owideu, aes(x = date, y = new_deaths)) +\n  geom_point(color = \"#FF2EAF\") +\n  labs(x = \"Date\", y = \"COVID Deaths in Europe (Daily)\") +\n  scale_x_date(\n    breaks = seq.Date(\n      from = as.Date(\"2020-01-01\"), \n      to = as.Date(\"2023-08-31\"), \n      by = \"7 weeks\"\n    ), \n    date_labels = \"%Y-%m\", \n    expand = c(0, 0)\n  ) +\n  scale_y_continuous(breaks = seq(0, 6500, by = 1000),\n                     labels = c(\"0\", \"1000\", \"\", \"3000\", \"\", \"5000\", \"\")) +\n  coord_cartesian(\n    xlim = as.Date(c(\"2020-01-01\", \"2023-08-31\")),\n    ylim = c(0, 7000)) +\n  theme_test() +\n  theme(\n    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),\n    axis.text.y = element_text(angle = 90),\n    axis.ticks.x = element_blank()\n  )"
  }
]